{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nipype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0e1b3ac91838>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnilearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mndimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspatial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnipype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterfaces\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mants\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeepdish\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nipype'"
     ]
    }
   ],
   "source": [
    "import nibabel as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from nibabel import processing as nbproc\n",
    "import nilearn\n",
    "from scipy import ndimage, spatial, stats\n",
    "from nipype.interfaces import ants\n",
    "import pandas\n",
    "import deepdish\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdir = '/mnt/c/Users/pgoodin/Desktop/tia_100_out/fmriprep/'\n",
    "mixing_matrix_fn = hdir+'sub-tia001/func/sub-tia001_task-rest_desc-MELODIC_mixing.tsv'\n",
    "noise_comps_fn = hdir+'sub-tia001/func/sub-tia001_task-rest_AROMAnoiseICs.csv'\n",
    "\n",
    "mm = np.genfromtxt(mixing_matrix_fn, delimiter = '\\t')\n",
    "n_comps = mm.shape[0]\n",
    "motion_idx = np.genfromtxt(noise_comps_fn, delimiter = ',').astype(int) -1 #ICA components start at 1, python is 0 indexed.\n",
    "n_motion_comps = motion_idx.shape[0]\n",
    "aroma_confounds = mm[:, motion_idx]\n",
    "aroma_confounds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: \n",
    "\n",
    "Wrap in Nipype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epi_lesion_confound(lesion_fn, melodic_fn, brain_mask_fn, thresh = 0.05):\n",
    "    '''\n",
    "    Finds melodic components with a high degree of overlap with lesion mask,\n",
    "    writes regressors for cleaning. Method outlined in\n",
    "    Yourganov et al. (2018). Removal of artifacts from resting-state fMRI data in stroke, Neuroimage Clin\n",
    "\n",
    "    Input:\n",
    "    lesion_fn - Path to lesion mask\n",
    "    melodic_fn - Path to melodic 4d output (found in working directory of fmriprep)\n",
    "    Note: This image is in 2mm space. Gets resliced to lesion mask dims as part of the process. You're welcome.\n",
    "    brain_mask_fn - Path to brain mask in functional space.\n",
    "    thresh - Threshold for similarity between lesion and component (default 0.05)\n",
    "\n",
    "    Output:\n",
    "    out_name - Path to csv file with components showing a high degree of overlap with lesion mask.\n",
    "    '''\n",
    "\n",
    "    out_path, out_name = os.path.split(lesion_fn)\n",
    "    out_name = '{}{}'.format(out_name.split('space')[0], 'LesionICs.csv')\n",
    "\n",
    "    lesion_hdr = nb.load(lesion_fn)\n",
    "    lesion_im = lesion_hdr.get_fdata()\n",
    "\n",
    "    ica_hdr = nb.load(melodic_fn)\n",
    "    ica_resampled = nilearn.image.resample_to_img(ica_hdr, lesion_hdr) #Reslice ica to lesion dimensions\n",
    "    ica_resampled_im = ica_resampled.get_fdata()\n",
    "\n",
    "    mask = nb.load(brain_mask_fn).get_fdata().astype(bool)\n",
    "\n",
    "    c_ji = [] #Component Jaccard Index\n",
    "\n",
    "    for n in range(0, ica_resampled_im.shape[-1]):\n",
    "        comp = ica_resampled_im[:, :, :, n].copy()\n",
    "        comp_flat = comp[mask]\n",
    "\n",
    "        dist_thresh = np.percentile(abs(comp_flat), 97.5)\n",
    "        comp_bool = (abs(comp_flat) >= dist_thresh).astype(int)\n",
    "        lesion_bool = (lesion_im[mask] > 0.1).astype(int) #Should be binary mask. This reduces any possible effects of normalisation (even though it's using NN...)\n",
    "\n",
    "        c_ji.append(1 - spatial.distance.jaccard(comp_bool, lesion_bool))\n",
    "\n",
    "    c_ji = np.array(c_ji)\n",
    "\n",
    "    remove_comps = pd.DataFrame(np.where(c_ji >= thresh)[0]).T #Quick and easy method to save in same format as fmriprep\n",
    "    print('Identified {} components associated with lesion'.format(remove_comps.size))\n",
    "\n",
    "    out_path, file_name = os.path.split(lesion_fn)\n",
    "    out_fn = '{}{}'.format(file_name.split('space')[0], 'LesionICs.csv')\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "    remove_comps.to_csv(out_name, sep = ',', header = False, index = False)\n",
    "\n",
    "    return(out_name)\n",
    "\n",
    "\n",
    "def epi_frist24(fmriprep_confounds_fn):\n",
    "    '''\n",
    "    Calculate the Taylor expansion of motion parameters ala Friston et al (year)\n",
    "\n",
    "    Inputs:\n",
    "    fmriprep_confouds_fn - Path to fmriprep confounds tsv file.\n",
    "\n",
    "    Outputs:\n",
    "    motion24 -  numpy array of 24 motion paramaters\n",
    "    '''\n",
    "\n",
    "    out_path, in_fn = os.path.split(fmriprep_confounds_fn)\n",
    "    out_fn = in_fn.split('desc-')[0] + 'desc-motion24.csv'\n",
    "\n",
    "\n",
    "    #CALCULATE FRISTON 24 MODEL (6 motion params + preceeding vol + each values squared.)\n",
    "    motion_df = pd.read_csv(fmriprep_confounds_fn, sep = '\\t')\n",
    "    motion_params = motion_df[['trans_x', 'trans_y', 'trans_z','rot_x', 'rot_y', 'rot_z']].values\n",
    "    motion_squared = motion_params ** 2\n",
    "    new_motion = np.concatenate((motion_params, motion_squared), axis = 1)\n",
    "    motion_roll = np.roll(motion_params, 1, axis = 0)\n",
    "    motion_roll[0] = 0\n",
    "    new_motion = np.concatenate((new_motion, motion_roll), axis = 1)\n",
    "    motion_roll_squared = motion_roll ** 2\n",
    "    motion24 = np.concatenate((new_motion, motion_roll_squared), axis = 1)\n",
    "\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "    np.savetxt(out_name, motion24, delimiter = ',')\n",
    "\n",
    "    return(out_name)\n",
    "\n",
    "\n",
    "\n",
    "def epi_lesion_confound(lesion_fn, melodic_fn, brain_mask_fn, thresh = 0.05):\n",
    "    '''\n",
    "    Finds melodic components with a high degree of overlap with lesion mask,\n",
    "    writes regressors for cleaning. Method outlined in\n",
    "    Yourganov et al. (2018). Removal of artifacts from resting-state fMRI data in stroke, Neuroimage Clin\n",
    "\n",
    "    Input:\n",
    "    lesion_fn - Path to lesion mask\n",
    "    melodic_fn - Path to melodic 4d output (found in working directory of fmriprep)\n",
    "    Note: This image is in 2mm space. Gets resliced to lesion mask dims as part of the process. You're welcome.\n",
    "    brain_mask_fn - Path to brain mask in functional space.\n",
    "    thresh - Threshold for similarity between lesion and component (default 0.05)\n",
    "\n",
    "    Output:\n",
    "    out_name - Path to csv file with components showing a high degree of overlap with lesion mask.\n",
    "    '''\n",
    "\n",
    "    out_path, out_name = os.path.split(lesion_fn)\n",
    "    out_name = '{}{}'.format(out_name.split('space')[0], 'LesionICs.csv')\n",
    "\n",
    "    lesion_hdr = nb.load(lesion_fn)\n",
    "    lesion_im = lesion_hdr.get_fdata()\n",
    "\n",
    "    ica_hdr = nb.load(melodic_fn)\n",
    "    ica_resampled = nilearn.image.resample_to_img(ica_hdr, lesion_hdr) #Reslice ica to lesion dimensions\n",
    "    ica_resampled_im = ica_resampled.get_fdata()\n",
    "\n",
    "    mask = nb.load(brain_mask_fn).get_fdata().astype(bool)\n",
    "\n",
    "    c_ji = [] #Component Jaccard Index\n",
    "\n",
    "    for n in range(0, ica_resampled_im.shape[-1]):\n",
    "        comp = ica_resampled_im[:, :, :, n].copy()\n",
    "        comp_flat = comp[mask]\n",
    "\n",
    "        dist_thresh = np.percentile(abs(comp_flat), 97.5)\n",
    "        comp_bool = (abs(comp_flat) >= dist_thresh).astype(int)\n",
    "        lesion_bool = (lesion_im[mask] > 0.1).astype(int) #Should be binary mask. This reduces any possible effects of normalisation (even though it's using NN...)\n",
    "\n",
    "        c_ji.append(1 - spatial.distance.jaccard(comp_bool, lesion_bool))\n",
    "\n",
    "    c_ji = np.array(c_ji)\n",
    "\n",
    "    remove_comps = pd.DataFrame(np.where(c_ji >= thresh)[0]).T #Quick and easy method to save in same format as fmriprep\n",
    "    print('Identified {} components associated with lesion'.format(remove_comps.size))\n",
    "\n",
    "    out_path, file_name = os.path.split(lesion_fn)\n",
    "    out_fn = '{}{}'.format(file_name.split('space')[0], 'LesionICs.csv')\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "    remove_comps.to_csv(out_name, sep = ',', header = False, index = False)\n",
    "\n",
    "    return(out_name)\n",
    "\n",
    "\n",
    "def epi_frist24(fmriprep_confounds_fn):\n",
    "    '''\n",
    "    Calculate the Taylor expansion of motion parameters ala Friston et al (year)\n",
    "\n",
    "    Inputs:\n",
    "    fmriprep_confouds_fn - Path to fmriprep confounds tsv file.\n",
    "\n",
    "    Outputs:\n",
    "    motion24 -  numpy array of 24 motion paramaters\n",
    "    '''\n",
    "\n",
    "    out_path, in_fn = os.path.split(fmriprep_confounds_fn)\n",
    "    out_fn = in_fn.split('desc-')[0] + 'desc-motion24.csv'\n",
    "\n",
    "\n",
    "    #CALCULATE FRISTON 24 MODEL (6 motion params + preceeding vol + each values squared.)\n",
    "    motion_df = pd.read_csv(fmriprep_confounds_fn, sep = '\\t')\n",
    "    motion_params = motion_df[['trans_x', 'trans_y', 'trans_z','rot_x', 'rot_y', 'rot_z']].values\n",
    "    motion_squared = motion_params ** 2\n",
    "    new_motion = np.concatenate((motion_params, motion_squared), axis = 1)\n",
    "    motion_roll = np.roll(motion_params, 1, axis = 0)\n",
    "    motion_roll[0] = 0\n",
    "    new_motion = np.concatenate((new_motion, motion_roll), axis = 1)\n",
    "    motion_roll_squared = motion_roll ** 2\n",
    "    motion24 = np.concatenate((new_motion, motion_roll_squared), axis = 1)\n",
    "\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "    np.savetxt(out_name, motion24, delimiter = ',')\n",
    "\n",
    "    return(out_name)\n",
    "\n",
    "\n",
    "\n",
    "def epi_collect_ICA_confounds(mixing_matrix_fn, noise_comps_fn, lesion_comps_fn  = None):\n",
    "    '''\n",
    "    Collects motion and lesion confounds using information in fmriprep output file <subj_id>_task-rest_AROMAnoiseICs.csv \n",
    "    and <subj_id>_task-rest_LesionICs.csv (if present).\n",
    "    \n",
    "    Extracts these from the mixing matrix file <subj_id>_task-rest_desc-MELODIC_mixing.tsv and writes to a new file.\n",
    "\n",
    "    Input:\n",
    "    mixing_matrix_fn - Path to mixing matrix\n",
    "    noise_comps_fn - Path to AROMA identified noise components\n",
    "    lesion_comps_fn - Path to epi_lesion_confound identified lesion components \n",
    "\n",
    "    Output:\n",
    "    out_name - Path to csv file with identified IC noise components\n",
    "    '''\n",
    "    \n",
    "    out_path, in_fn = os.path.split(mixing_matrix_fn)\n",
    "    out_fn = in_fn.split('desc-')[0] + 'desc-ICA_confounds.csv'\n",
    "\n",
    "    mm = np.genfromtxt(mixing_matrix_fn, delimiter = '\\t')\n",
    "    n_comps = mm.shape[1]\n",
    "\n",
    "    comp_idx = np.genfromtxt(noise_comps_fn, delimiter = ',').astype(int) -1 #ICA components start at 1, python is 0 indexed.\n",
    "    if lesion_comps_fn: #Check for overlap between AROMA and lesion identified components\n",
    "        lesion_idx = np.genfromtxt(lesion_comps_fn, delimiter = ',').astype(int)\n",
    "        merge_idx = np.append(motion_idx, lesion_idx)\n",
    "        comp_idx = np.unique(merge_idx)\n",
    "\n",
    "    n_noise_comps = comp_idx.shape[0]\n",
    "    aroma_confounds = mm[:, comp_idx]\n",
    "\n",
    "    print('Number of motion components to remove is: {} of {} ({}%)\\n{}'.format(n_noise_comps, \n",
    "                                                                            n_comps, \n",
    "                                                                            (n_noise_comps / n_comps) * 100, \n",
    "                                                                            comp_idx))\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "    np.savetxt(out_name, aroma_confounds, delimiter = ',')\n",
    "    \n",
    "    return(out_name)\n",
    "\n",
    "\n",
    "def epi_gen_confounds_matrix(fmriprep_confounds_fn, confound_list):\n",
    "    '''\n",
    "    Create the confounds matrix for regression using non-melodic output from fmriprep.\n",
    "\n",
    "    Inputs:\n",
    "    fmriprep_confouds_fn - Path to fmriprep confounds tsv file.\n",
    "    confound_list - List of confounds to be used (note: must match column names in fmriprep confounds tsv)\n",
    "    '''\n",
    "\n",
    "    out_path, in_fn = os.path.split(fmriprep_confounds_fn)\n",
    "    out_fn = in_fn.split('desc-')[0] + 'desc-fmriprep_seleced_confounds.csv'\n",
    "\n",
    "    df = pd.read_csv(fmriprep_confounds_fn, sep = '\\t')\n",
    "    confounds_vals = df[confound_list].values\n",
    "\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "    np.savetxt(out_name, confounds_vals, delimiter = ',')\n",
    "    return(out_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def epi_clean(epi_fn, mask_fn, selected_confounds_fn, aroma_confounds_fn, add_const = True, output_z = False):\n",
    "    '''\n",
    "    Uses OLS GLM to remove confounds from functional time series. Follows the fsl_regfilt method of\n",
    "    demeaning functional and confounds, then adding mean back to functional after regression.\n",
    "\n",
    "    Inputs:\n",
    "    epi_fn - Path to epi file\n",
    "    mask_fn - Path to binary brain mask\n",
    "    selected_confounds_fn - Path to output file from epi_gen_confounds_matrix\n",
    "    confounds_fn - Path to text file (no delimiters) with timeseries of each confound as a column\n",
    "    output_z - Bool. Output z scored, cleaned epi image also\n",
    "\n",
    "    Outputs:\n",
    "    cleaned_im - Epi with confounds removed\n",
    "    cleaned_im_z - As above, but z scored.\n",
    "    '''\n",
    "\n",
    "    fmriprep_confounds = np.genfromtxt(selected_confounds_fn, delimiter = ',')\n",
    "    aroma_confounds = np.genfromtxt(aroma_confounds_fn, delimiter = ',')\n",
    "\n",
    "    if add_const:\n",
    "        lin = np.arange(0, fmriprep_confounds.shape[0])\n",
    "        lin = lin[:, np.newaxis]\n",
    "        X = np.hstack((fmriprep_confounds, aroma_confounds, lin))\n",
    "        \n",
    "    else:\n",
    "        X = np.hstack((fmriprep_confounds, aroma_confounds))\n",
    "    \n",
    "    X_demeaned = X - X.mean()\n",
    "\n",
    "\n",
    "    print('Removing {} confounds'.format(X_demeaned.shape[-1]))\n",
    "\n",
    "    hdr = nb.load(epi_fn)\n",
    "    im = hdr.get_fdata()\n",
    "\n",
    "    brain_mask = nb.load(mask_fn).get_fdata().astype(bool)\n",
    "\n",
    "    y = im[brain_mask].T\n",
    "    y_mean = im[brain_mask].T.mean()\n",
    "    y_demeaned = y - y_mean\n",
    "\n",
    "    b = np.linalg.lstsq(X_demeaned, y_demeaned, rcond =  None)[0]\n",
    "    y_resid = y - X.dot(b)\n",
    "    y_resid = y_resid + y_mean\n",
    "\n",
    "    #Replaces min / max intensities from header with 0 (similar to fsl GLM)\n",
    "    hdr.header['cal_min'] = 0\n",
    "    hdr.header['cal_max'] = 0\n",
    "\n",
    "    cleaned_im = np.zeros_like(im)\n",
    "    cleaned_im[brain_mask] = y_resid.T\n",
    "\n",
    "    out_path, in_fn = os.path.split(epi_fn)\n",
    "    out_fn = in_fn.split('.')[0].split('bold')[0] + 'cleaned.nii.gz'\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "\n",
    "    nb.Nifti1Image(cleaned_im, header = hdr.header, affine = hdr.affine).to_filename(out_name)\n",
    "\n",
    "    if output_z:\n",
    "        del cleaned_im #Save RAM\n",
    "\n",
    "        cleaned_im_z = np.zeros_like(im)\n",
    "        cleaned_im_z[brain_mask] = (y_resid.T - y_resid.T.mean()) / y_resid.T.std()\n",
    "\n",
    "        out_fn_z = in_fn.split('.')[0].split('bold')[0] + 'cleaned_z.nii.gz'\n",
    "        out_name_z = os.path.join(out_path, out_fn_z)\n",
    "\n",
    "        nb.Nifti1Image(cleaned_im_z, header = hdr.header, affine = hdr.affine).to_filename(out_name_z)\n",
    "\n",
    "    return(out_name, out_name_z, X_demeaned if output_z else out_name, X_demeaned)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def epi_smooth(epi_fn, mask_fn, fwhm = None):\n",
    "    '''\n",
    "    Smooth epi with a Gaussian kernel\n",
    "\n",
    "    Inputs:\n",
    "    epi_fn - Path to epi file\n",
    "    fwhm - int. Full width half maximum (in mm) of kernel to smooth epi file\n",
    "\n",
    "    Outputs:\n",
    "    smoothed_epi - Smoothed epi file to fwhm\n",
    "    '''\n",
    "\n",
    "    hdr = nb.load(epi_fn)\n",
    "\n",
    "    brain_mask = nb.load(mask_fn).get_fdata().astype(bool)\n",
    "\n",
    "    if not fwhm:\n",
    "        fwhm = 0\n",
    "        print('\\n\\n***WARNING***\\nKernel FWHM not set!\\n\\n')\n",
    "\n",
    "    print('Smoothing with kernel size: {}'.format(fwhm))\n",
    "\n",
    "    smoothed_flat = nbproc.smooth_image(img = hdr, fwhm = fwhm).get_fdata()[brain_mask]\n",
    "    smoothed_epi = np.zeros_like(hdr.get_fdata())\n",
    "    smoothed_epi[brain_mask] = smoothed_flat\n",
    "\n",
    "    out_path, in_fn = os.path.split(epi_fn)\n",
    "    out_fn = in_fn.split('.')[0].split('bold')[0] + '_smoothed_{}mm.nii.gz'.format(fwhm)\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "\n",
    "    nb.Nifti1Image(smoothed_epi, header = hdr.header, affine = hdr.affine).to_filename(out_name)\n",
    "\n",
    "    return(out_name)\n",
    "\n",
    "def anat_apply_transforms(moving_fn, ref_fn, transform_fn, trans_type = 'desc'):\n",
    "    '''\n",
    "    Applies transforms calculated by ANTS\n",
    "\n",
    "    Input:\n",
    "    moving_fn - Path to file to be transformed\n",
    "    ref_fn - Path to reference image (eg. if transform is from subj > MNI, reference is MNI template)\n",
    "    transform_fn - The transform output from ANTs (usually in h5 format)\n",
    "    trans_type - String of either desc or label. Desc denotes an anatomical scan, label a labelled image (eg. lesion mask, tissue prob mask)\n",
    "\n",
    "    Output:\n",
    "    out_fn - Path to transformed image\n",
    "    '''\n",
    "\n",
    "    space = re.findall('to-((.*)_(.*_))', transform_fn)[0][1] #Find which space transform is to\n",
    "    out_path, trans_name = os.path.split(transform_fn) #Places output in same directory as transform file\n",
    "\n",
    "    in_fn = os.path.split(moving_fn)[-1]\n",
    "    if trans_type == 'desc':\n",
    "        pre_split, post_split = in_fn.split('desc-')\n",
    "        out_fn = pre_split + 'space-' + space + '_desc-' + post_split\n",
    "\n",
    "    elif trans_type == 'label':\n",
    "        pre_split, post_split = in_fn.split('label-')\n",
    "        out_fn = pre_split + 'space-' + space + '_label-' + post_split\n",
    "\n",
    "    else:\n",
    "        raise Exception('Only desc or label identifiers are currently supported')\n",
    "\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "    print(out_name)\n",
    "\n",
    "    subj2mni = ants.ApplyTransforms()\n",
    "    subj2mni.inputs.input_image = moving_fn\n",
    "    subj2mni.inputs.reference_image = ref_fn\n",
    "    subj2mni.inputs.transforms = transform_fn\n",
    "    if trans_type == 'desc':\n",
    "        subj2mni.inputs.interpolation = 'Linear'\n",
    "    else:\n",
    "        subj2mni.inputs.interpolation = 'NearestNeighbor' #Don't want mask smoothed!\n",
    "    subj2mni.inputs.output_image = out_name\n",
    "\n",
    "    return(out_name)\n",
    "\n",
    "\n",
    "def anat_gm_mask(dseg_fn, vox_size = None):\n",
    "    '''\n",
    "    Uses dseg (tissue segmentation) output from freesurfer and creates a grey matter mask.\n",
    "\n",
    "    Input:\n",
    "    dseg_fn - Path to anatomical space dset\n",
    "    vox_size - Optional resample in mm (can be int or list of ints)\n",
    "\n",
    "    Output:\n",
    "    out_name - Path to grey matter mask\n",
    "    out_name_ds - Path to downsampled grey matter mask (typically from anatomical to functional)\n",
    "    '''\n",
    "\n",
    "    out_path, in_fn = os.path.split(dseg_fn)\n",
    "    out_fn = in_fn.split('dseg')[0] + 'desc-gm_mask.nii.gz'\n",
    "\n",
    "    hdr = nb.load(dseg_fn)\n",
    "    dseg_im = hdr.get_data()\n",
    "    gm_mask = dseg_im == 2\n",
    "\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "    nb_image = nb.Nifti1Image(gm_mask, header = hdr.header, affine = hdr.affine)\n",
    "    nb_image.to_filename(out_name)\n",
    "\n",
    "    if vox_size:\n",
    "        ds = nb.processing.resample_to_output(hdr, vox_size)\n",
    "        ds_data = ds.get_data() == 2\n",
    "        if type(vox_size) == list:\n",
    "            vox = vox_size[0]\n",
    "        else:\n",
    "            #Figure what's going on below...\n",
    "            vox = vox_size\n",
    "        out_fn_ds = in_fn.split('dseg')[0] + 'desc-gm_mask_voxsize_{}mm.nii.gz'.format(vox_size)\n",
    "        out_name_ds = os.path.join(out_path, out_fn_ds).replace('anat', 'func')\n",
    "        nb.Nifti1Image(ds_data, header = ds.header, affine = ds.affine).to_filename(out_name_ds)\n",
    "\n",
    "\n",
    "    return(out_name, out_name_ds if vox_size else out_name)\n",
    "\n",
    "\n",
    "def anat2epi_ds(anat_fn, epi_3d_fn):\n",
    "    '''\n",
    "    Downsamples anatomical image to the size of the functional image\n",
    "    Note: This is used primarily for downsampling lesion masks to functional space\n",
    "\n",
    "    Input:\n",
    "    anat_fn - Path to anatomical image to downsample to functional space\n",
    "    epi_3d_fn - Path to 3d epi image that contains shape information. Could have been 4d, but I was lazy.\n",
    "\n",
    "    Output:\n",
    "    out_name - Path to downsampled anatomical image in functional dimensions\n",
    "    '''\n",
    "\n",
    "    out_path, epi_name = os.path.split(epi_3d_fn) #Places output in func directory\n",
    "    task = re.findall('task-(.*?_)', epi_3d_fn)[0][:-1] #output is TaskType_\n",
    "    print(task)\n",
    "\n",
    "    epi_hdr = nb.load(epi_3d_fn)\n",
    "    vox_size = np.average(epi_hdr.header.get_zooms()).astype(int)\n",
    "\n",
    "    in_fn = os.path.split(anat_fn)[-1]\n",
    "\n",
    "    item_list = in_fn.split('T1w')\n",
    "    item_list.insert(1, 'task-{}'.format(task))\n",
    "    out_fn = ''.join(item_list)\n",
    "\n",
    "    hdr = nb.load(anat_fn)\n",
    "\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "\n",
    "    #Downsample\n",
    "    anat_im_ds = nb.processing.resample_to_output(hdr, vox_size)\n",
    "    nb.Nifti1Image(anat_im_ds.get_fdata(), header = anat_im_ds.header, affine = anat_im_ds.affine).to_filename(out_name)\n",
    "\n",
    "    return(out_name)\n",
    "\n",
    "\n",
    "def epi_parcellation(epi_fn, parc_fn, parc_ids_fn, atlas_name = 'aal'):\n",
    "\n",
    "    '''\n",
    "    Produces parcellation of functional data.\n",
    "    Note: Functional and parcellation files must be the same dimensions.\n",
    "\n",
    "    Input:\n",
    "    epi_fn - Path to functional file to be parcellated\n",
    "    parc_fn - Path to parcellation file. Should be a single 3d volume with unique IDs per parcellation\n",
    "    parc_ids_fn - Path to text file with either numerical or anatomical labels for each parcellation\n",
    "    atlas_name - Name of the atlas used for parcellataion\n",
    "\n",
    "    Output:\n",
    "    parc_vox_fn - Path to dictionary (in hdf5 form) that contains each voxel's time series from a parcellation\n",
    "    parc_ts_fn - Path to .tsv file with mean timeseries from each parcellation region.\n",
    "    '''\n",
    "\n",
    "    out_fn_prefix = epi_fn.split('space')[0]\n",
    "    out_fn_suffix = 'atlas-{}_timeseries'.format(atlas_name)\n",
    "    out_fn_stem = '{}{}'.format(out_fn_prefix, out_fn_suffix)\n",
    "\n",
    "    parc_vox = {}\n",
    "\n",
    "    parc_ids = []\n",
    "    with open(parc_ids_fn) as ids:\n",
    "        for line in ids:\n",
    "            parc_ids.append(line[:-1].lower()) #Remove newline\n",
    "\n",
    "    hdr = nb.load(epi_fn)\n",
    "    epi_data = hdr.get_fdata()\n",
    "\n",
    "    parc = nb.load(parc_fn).get_fdata()\n",
    "    parc_vals = np.unique(parc)[1:].astype(int)\n",
    "\n",
    "    id_zip = zip(parc_vals, parc_ids)\n",
    "\n",
    "    if epi_data.shape[:-1] != parc.shape:\n",
    "        raise Exception('EPI and parcellation must be in same space')\n",
    "    else:\n",
    "        parc_ts = {}\n",
    "\n",
    "    for n, (parc_n, parc_id) in enumerate(id_zip):\n",
    "        roi_ts = epi_data[(parc == parc_n)]\n",
    "\n",
    "        parc_vox[parc_id] = roi_ts\n",
    "        parc_ts[parc_id] = roi_ts.mean(0)\n",
    "\n",
    "\n",
    "\n",
    "    parc_vox_fn = '{}_parc_vox.h5'.format(out_fn_stem)\n",
    "    parc_ts_fn = '{}_mean.tsv'.format(out_fn_stem)\n",
    "\n",
    "    deepdish.io.save(parc_vox_fn, parc_vox)\n",
    "    pd.DataFrame(parc_ts).to_csv(parc_ts_fn, sep = '\\t', index = False)\n",
    "\n",
    "    return(parc_ts_fn)\n",
    "\n",
    "\n",
    "def parc_lesion_overlap(parc_fn, lesion_fn, parc_ids_fn, atlas_name = 'aal'):\n",
    "    '''\n",
    "    Calculates overlap between lesion mask and parcellation atlas\n",
    "\n",
    "    Input:\n",
    "    parc_fn - Path to parcellation file. Should be a single 3d volume with unique IDs per parcellation\n",
    "    lesion_fn - Path to lesion mask\n",
    "    parc_ids_fn - Path to text file with either numerical or anatomical labels for each parcellation\n",
    "    atlas_name - Name of the atlas used for parcellataion\n",
    "\n",
    "    Output:\n",
    "    out_fn - Path to tsv file showing amount of damage per parcel in sum of voxels and percentage of voxels\n",
    "    '''\n",
    "\n",
    "    out_fn_prefix = lesion_fn.split('space')[0]\n",
    "    out_fn_suffix = 'atlas-{}_desc-lesion_damage.tsv'.format(atlas_name)\n",
    "    out_fn = '{}{}'.format(out_fn_prefix, out_fn_suffix)\n",
    "\n",
    "    parc_ids = []\n",
    "    with open(parc_ids_fn) as ids:\n",
    "        for line in ids:\n",
    "            parc_ids.append(line[:-1].lower()) #Remove newline\n",
    "\n",
    "    parc = nb.load(parc_fn).get_fdata()\n",
    "    parc_vals = np.unique(parc)[1:].astype(int)\n",
    "\n",
    "    lesion = nb.load(lesion_fn).get_fdata()\n",
    "\n",
    "    if lesion.shape != parc.shape:\n",
    "        raise Exception('Lesion mask and parcellation must be in same space')\n",
    "    else:\n",
    "        parc_damage = {}\n",
    "\n",
    "    id_zip = zip(parc_vals, parc_ids)\n",
    "\n",
    "    for n, (parc_n, parc_id) in enumerate(id_zip):\n",
    "        parc_size = (parc == parc_n).sum()\n",
    "        parc_damage[parc_id] = [int(lesion[parc == parc_n].sum()), abs(round(lesion[parc == parc_n].sum() / parc_size, 2))]\n",
    "\n",
    "    parc_damage_df = pd.DataFrame(parc_damage).T\n",
    "    parc_damage_df.columns = ['parc', 'sum', 'percent']\n",
    "    parc_damage_df.to_csv(out_fn, sep = '\\t')\n",
    "\n",
    "    return(out_fn)\n",
    "\n",
    "\n",
    "def epi_calc_lag(epi_fn, gm_mask_fn, brain_mask_fn):\n",
    "    '''\n",
    "    Compute BOLD lag map (ala *insert paper here*).\n",
    "    '''\n",
    "\n",
    "    brain_mask = nb.load(brain_mask_fn).get_data().astype(bool)\n",
    "    gm_mask = nb.load(out_name_ds).get_data().astype(bool)\n",
    "\n",
    "    hdr = nb.load(epi_fn)\n",
    "    epi_im = hdr.get_data()\n",
    "    epi_ts = epi_im[brain_mask]\n",
    "    tr = hdr.header.get_zooms()[-1]\n",
    "\n",
    "    global_ts = epi_im[gm_mask].mean(0)\n",
    "\n",
    "    epi_cross_corr = np.zeros(epi_ts.shape[0])\n",
    "    nsamples = 1000\n",
    "\n",
    "\n",
    "    out_path, in_fn = os.path.split(epi_fn)\n",
    "    out_fn = in_fn.split('desc')[0] + 'desc-lag_map.nii.gz'\n",
    "\n",
    "\n",
    "    for n, vox in enumerate(epi_ts):\n",
    "        vox_zero_mean = vox - vox.mean(axis = 0)\n",
    "        global_ts_zero_mean = global_ts - global_ts.mean(axis = 0)\n",
    "\n",
    "        #Full cross correlation\n",
    "    #     c = np.correlate(vox_zero_mean, global_ts_zero_mean, 'same') #Correlation of all shifts of time series.\n",
    "        c = np.correlate(vox, global_ts, 'full') #Correlation of all shifts of time series.\n",
    "        zero_lag = np.floor(c.shape[0] / 2).astype(int)\n",
    "        c_culled = c[zero_lag - 3: zero_lag + 4] #Capture +- 3 TRs (9 seconds)\n",
    "        maxC_idx = np.where(c == c_culled.max())[0].item() #Find where the maximum value of the windowed correlation exists within the larger cross correlation\n",
    "\n",
    "        #Sectioned \n",
    "        y = c[maxC_idx - 1: maxC_idx + 2] #Collect max +- 1 points for polynomial fit\n",
    "        x = np.arange(0, len(y)) #Split 2d array into 1d\n",
    "        z = np.polyfit(x, y, 2) #Second decree (parabolic) polynomial fit w/ coefs\n",
    "        p = np.poly1d(z) # Calculate discrete vals\n",
    "        xp = np.linspace(0, 2, nsamples) # Create array to house sample values\n",
    "        interp = p(xp) # Calculate nsamples of poly func\n",
    "        x_new = np.linspace(tr * -1, tr, p(xp).shape[0]) # Generate lag time series\n",
    "        maxP_idx = np.where(interp == interp.max())[0].item() #Get max lag from interpolated time series\n",
    "\n",
    "        #Putting it all back together\n",
    "        lag_point = zero_lag - maxC_idx\n",
    "        lag_point_seconds = tr * lag_point\n",
    "        lag_val = np.round(lag_point_seconds + x_new[maxP_idx], 2)\n",
    "\n",
    "        epi_cross_corr[n] = lag_val\n",
    "\n",
    "    lag_map = np.zeros_like(brain_mask.astype(float))\n",
    "    lag_map[brain_mask] = epi_cross_corr\n",
    "\n",
    "    out_name = os.path.join(out_path, out_fn)\n",
    "\n",
    "    lag_nii = nb.Nifti1Image(lag_map, header = hdr.header, affine = hdr.affine)\n",
    "    lag_nii.to_filename(out_name)\n",
    "\n",
    "    return(out_name)\n",
    "#     '''\n",
    "#     Collects motion and lesion confounds using information in fmriprep output file <subj_id>_task-rest_AROMAnoiseICs.csv \n",
    "#     and <subj_id>_task-rest_LesionICs.csv (if present).\n",
    "    \n",
    "#     Extracts these from the mixing matrix file <subj_id>_task-rest_desc-MELODIC_mixing.tsv and writes to a new file.\n",
    "\n",
    "#     Input:\n",
    "#     mixing_matrix_fn - Path to mixing matrix\n",
    "#     noise_comps_fn - Path to AROMA identified noise components\n",
    "#     lesion_comps_fn - Path to epi_lesion_confound identified lesion components \n",
    "\n",
    "#     Output:\n",
    "#     out_name - Path to csv file with identified IC noise components\n",
    "#     '''\n",
    "    \n",
    "#     out_path, in_fn = os.path.split(mixing_matrix_fn)\n",
    "#     out_fn = in_fn.split('desc-')[0] + 'desc-ICA_confounds.csv'\n",
    "\n",
    "#     mm = np.genfromtxt(mixing_matrix_fn, delimiter = '\\t')\n",
    "#     n_comps = mm.shape[1]\n",
    "\n",
    "#     comp_idx = np.genfromtxt(noise_comps_fn, delimiter = ',').astype(int) -1 #ICA components start at 1, python is 0 indexed.\n",
    "#     if lesion_comps_fn: #Check for overlap between AROMA and lesion identified components\n",
    "#         lesion_idx = np.genfromtxt(lesion_comps_fn, delimiter = ',').astype(int)\n",
    "#         merge_idx = np.append(motion_idx, lesion_idx)\n",
    "#         comp_idx = np.unique(merge_idx)\n",
    "\n",
    "#     n_noise_comps = comp_idx.shape[0]\n",
    "#     aroma_confounds = mm[:, comp_idx]\n",
    "\n",
    "#     print('Number of motion components to remove is: {} of {} ({}%)\\n{}'.format(n_noise_comps, \n",
    "#                                                                             n_comps, \n",
    "#                                                                             (n_noise_comps / n_comps) * 100, \n",
    "#                                                                             comp_idx))\n",
    "#     out_name = os.path.join(out_path, out_fn)\n",
    "#     np.savetxt(out_name, aroma_confounds, delimiter = ',')\n",
    "    \n",
    "#     return(out_name)\n",
    "\n",
    "\n",
    "# def epi_gen_confounds_matrix(fmriprep_confounds_fn, confound_list):\n",
    "#     '''\n",
    "#     Create the confounds matrix for regression using non-melodic output from fmriprep.\n",
    "\n",
    "#     Inputs:\n",
    "#     fmriprep_confouds_fn - Path to fmriprep confounds tsv file.\n",
    "#     confound_list - List of confounds to be used (note: must match column names in fmriprep confounds tsv)\n",
    "#     '''\n",
    "\n",
    "#     out_path, in_fn = os.path.split(fmriprep_confounds_fn)\n",
    "#     out_fn = in_fn.split('desc-')[0] + 'desc-fmriprep_seleced_confounds.csv'\n",
    "\n",
    "#     df = pd.read_csv(fmriprep_confounds_fn, sep = '\\t')\n",
    "#     confounds_vals = df[confound_list].values\n",
    "\n",
    "#     out_name = os.path.join(out_path, out_fn)\n",
    "#     np.savetxt(out_name, confounds_vals, delimiter = ',')\n",
    "#     return(out_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def epi_clean(epi_fn, mask_fn, selected_confounds_fn, aroma_confounds_fn, add_const = True, output_z = False):\n",
    "#     '''\n",
    "#     Uses OLS GLM to remove confounds from functional time series. Follows the fsl_regfilt method of\n",
    "#     demeaning functional and confounds, then adding mean back to functional after regression.\n",
    "\n",
    "#     Inputs:\n",
    "#     epi_fn - Path to epi file\n",
    "#     mask_fn - Path to binary brain mask\n",
    "#     selected_confounds_fn - Path to output file from epi_gen_confounds_matrix\n",
    "#     confounds_fn - Path to text file (no delimiters) with timeseries of each confound as a column\n",
    "#     output_z - Bool. Output z scored, cleaned epi image also\n",
    "\n",
    "#     Outputs:\n",
    "#     cleaned_im - Epi with confounds removed\n",
    "#     cleaned_im_z - As above, but z scored.\n",
    "#     '''\n",
    "\n",
    "#     fmriprep_confounds = np.genfromtxt(selected_confounds_fn, delimiter = ',')\n",
    "#     aroma_confounds = np.genfromtxt(aroma_confounds_fn, delimiter = ',')\n",
    "\n",
    "#     if add_const:\n",
    "#         lin = np.arange(0, fmriprep_confounds.shape[0])\n",
    "#         lin = lin[:, np.newaxis]\n",
    "#         X = np.hstack((fmriprep_confounds, aroma_confounds, lin))\n",
    "        \n",
    "#     else:\n",
    "#         X = np.hstack((fmriprep_confounds, aroma_confounds))\n",
    "    \n",
    "#     X_demeaned = X - X.mean()\n",
    "\n",
    "\n",
    "#     print('Removing {} confounds'.format(X_demeaned.shape[-1]))\n",
    "\n",
    "#     hdr = nb.load(epi_fn)\n",
    "#     im = hdr.get_fdata()\n",
    "\n",
    "#     brain_mask = nb.load(mask_fn).get_fdata().astype(bool)\n",
    "\n",
    "#     y = im[brain_mask].T\n",
    "#     y_mean = im[brain_mask].T.mean()\n",
    "#     y_demeaned = y - y_mean\n",
    "\n",
    "#     b = np.linalg.lstsq(X_demeaned, y_demeaned, rcond =  None)[0]\n",
    "#     y_resid = y - X.dot(b)\n",
    "#     y_resid = y_resid + y_mean\n",
    "\n",
    "#     #Replaces min / max intensities from header with 0 (similar to fsl GLM)\n",
    "#     hdr.header['cal_min'] = 0\n",
    "#     hdr.header['cal_max'] = 0\n",
    "\n",
    "#     cleaned_im = np.zeros_like(im)\n",
    "#     cleaned_im[brain_mask] = y_resid.T\n",
    "\n",
    "#     out_path, in_fn = os.path.split(epi_fn)\n",
    "#     out_fn = in_fn.split('.')[0].split('bold')[0] + 'cleaned.nii.gz'\n",
    "#     out_name = os.path.join(out_path, out_fn)\n",
    "\n",
    "#     nb.Nifti1Image(cleaned_im, header = hdr.header, affine = hdr.affine).to_filename(out_name)\n",
    "\n",
    "#     if output_z:\n",
    "#         del cleaned_im #Save RAM\n",
    "\n",
    "#         cleaned_im_z = np.zeros_like(im)\n",
    "#         cleaned_im_z[brain_mask] = (y_resid.T - y_resid.T.mean()) / y_resid.T.std()\n",
    "\n",
    "#         out_fn_z = in_fn.split('.')[0].split('bold')[0] + 'cleaned_z.nii.gz'\n",
    "#         out_name_z = os.path.join(out_path, out_fn_z)\n",
    "\n",
    "#         nb.Nifti1Image(cleaned_im_z, header = hdr.header, affine = hdr.affine).to_filename(out_name_z)\n",
    "\n",
    "#     return(out_name, out_name_z, X_demeaned if output_z else out_name, X_demeaned)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def epi_smooth(epi_fn, mask_fn, fwhm = None):\n",
    "#     '''\n",
    "#     Smooth epi with a Gaussian kernel\n",
    "\n",
    "#     Inputs:\n",
    "#     epi_fn - Path to epi file\n",
    "#     fwhm - int. Full width half maximum (in mm) of kernel to smooth epi file\n",
    "\n",
    "#     Outputs:\n",
    "#     smoothed_epi - Smoothed epi file to fwhm\n",
    "#     '''\n",
    "\n",
    "#     hdr = nb.load(epi_fn)\n",
    "\n",
    "#     brain_mask = nb.load(mask_fn).get_fdata().astype(bool)\n",
    "\n",
    "#     if not fwhm:\n",
    "#         fwhm = 0\n",
    "#         print('\\n\\n***WARNING***\\nKernel FWHM not set!\\n\\n')\n",
    "\n",
    "#     print('Smoothing with kernel size: {}'.format(fwhm))\n",
    "\n",
    "#     smoothed_flat = nbproc.smooth_image(img = hdr, fwhm = fwhm).get_fdata()\n",
    "#     smoothed_epi = np.zeros_like(hdr.get_fdata())\n",
    "#     smoothed_epi[brain_mask] = smoothed_flat\n",
    "\n",
    "#     out_path, in_fn = os.path.split(epi_fn)\n",
    "#     out_fn = in_fn.split('.')[0].split('bold')[0] + '_smoothed_{}mm.nii.gz'.format(fwhm)\n",
    "#     out_name = os.path.join(out_path, out_fn)\n",
    "\n",
    "#     nb.Nifti1Image(smoothed_epi, header = hdr.header, affine = hdr.affine).to_filename(out_name)\n",
    "\n",
    "#     return(out_name)\n",
    "\n",
    "# def anat_apply_transforms(moving_fn, ref_fn, transform_fn, trans_type = 'desc'):\n",
    "#     '''\n",
    "#     Applies transforms calculated by ANTS\n",
    "\n",
    "#     Input:\n",
    "#     moving_fn - Path to file to be transformed\n",
    "#     ref_fn - Path to reference image (eg. if transform is from subj > MNI, reference is MNI template)\n",
    "#     transform_fn - The transform output from ANTs (usually in h5 format)\n",
    "#     trans_type - String of either desc or label. Desc denotes an anatomical scan, label a labelled image (eg. lesion mask, tissue prob mask)\n",
    "\n",
    "#     Output:\n",
    "#     out_fn - Path to transformed image\n",
    "#     '''\n",
    "\n",
    "#     space = re.findall('to-((.*)_(.*_))', transform_fn)[0][1] #Find which space transform is to\n",
    "#     out_path, trans_name = os.path.split(transform_fn) #Places output in same directory as transform file\n",
    "\n",
    "#     in_fn = os.path.split(moving_fn)[-1]\n",
    "#     if trans_type == 'desc':\n",
    "#         pre_split, post_split = in_fn.split('desc-')\n",
    "#         out_fn = pre_split + 'space-' + space + '_desc-' + post_split\n",
    "\n",
    "#     elif trans_type == 'label':\n",
    "#         pre_split, post_split = in_fn.split('label-')\n",
    "#         out_fn = pre_split + 'space-' + space + '_label-' + post_split\n",
    "\n",
    "#     else:\n",
    "#         raise Exception('Only desc or label identifiers are currently supported')\n",
    "\n",
    "#     out_name = os.path.join(out_path, out_fn)\n",
    "#     print(out_name)\n",
    "\n",
    "#     subj2mni = ants.ApplyTransforms()\n",
    "#     subj2mni.inputs.input_image = moving_fn\n",
    "#     subj2mni.inputs.reference_image = ref_fn\n",
    "#     subj2mni.inputs.transforms = transform_fn\n",
    "#     if trans_type == 'desc':\n",
    "#         subj2mni.inputs.interpolation = 'Linear'\n",
    "#     else:\n",
    "#         subj2mni.inputs.interpolation = 'NearestNeighbor' #Don't want mask smoothed!\n",
    "#     subj2mni.inputs.output_image = out_name\n",
    "\n",
    "#     return(out_name)\n",
    "\n",
    "\n",
    "# def anat_gm_mask(dseg_fn, vox_size = None):\n",
    "#     '''\n",
    "#     Uses dseg (tissue segmentation) output from freesurfer and creates a grey matter mask.\n",
    "\n",
    "#     Input:\n",
    "#     dseg_fn - Path to anatomical space dset\n",
    "#     vox_size - Optional resample in mm (can be int or list of ints)\n",
    "\n",
    "#     Output:\n",
    "#     out_name - Path to grey matter mask\n",
    "#     out_name_ds - Path to downsampled grey matter mask (typically from anatomical to functional)\n",
    "#     '''\n",
    "\n",
    "#     out_path, in_fn = os.path.split(dseg_fn)\n",
    "#     out_fn = in_fn.split('dseg')[0] + 'desc-gm_mask.nii.gz'\n",
    "\n",
    "#     hdr = nb.load(dseg_fn)\n",
    "#     dseg_im = hdr.get_data()\n",
    "#     gm_mask = dseg_im == 2\n",
    "\n",
    "#     out_name = os.path.join(out_path, out_fn)\n",
    "#     nb_image = nb.Nifti1Image(gm_mask, header = hdr.header, affine = hdr.affine)\n",
    "#     nb_image.to_filename(out_name)\n",
    "\n",
    "#     if vox_size:\n",
    "#         ds = nb.processing.resample_to_output(hdr, vox_size)\n",
    "#         ds_data = ds.get_data() == 2\n",
    "#         if type(vox_size) == list:\n",
    "#             vox = vox_size[0]\n",
    "#         else:\n",
    "#             #Figure what's going on below...\n",
    "#             vox = vox_size\n",
    "#         out_fn_ds = in_fn.split('dseg')[0] + 'desc-gm_mask_voxsize_{}mm.nii.gz'.format(vox_size)\n",
    "#         out_name_ds = os.path.join(out_path, out_fn_ds).replace('anat', 'func')\n",
    "#         nb.Nifti1Image(ds_data, header = ds.header, affine = ds.affine).to_filename(out_name_ds)\n",
    "\n",
    "\n",
    "#     return(out_name, out_name_ds if vox_size else out_name)\n",
    "\n",
    "\n",
    "# def anat2epi_ds(anat_fn, epi_3d_fn):\n",
    "#     '''\n",
    "#     Downsamples anatomical image to the size of the functional image\n",
    "#     Note: This is used primarily for downsampling lesion masks to functional space\n",
    "\n",
    "#     Input:\n",
    "#     anat_fn - Path to anatomical image to downsample to functional space\n",
    "#     epi_3d_fn - Path to 3d epi image that contains shape information. Could have been 4d, but I was lazy.\n",
    "\n",
    "#     Output:\n",
    "#     out_name - Path to downsampled anatomical image in functional dimensions\n",
    "#     '''\n",
    "\n",
    "#     out_path, epi_name = os.path.split(epi_3d_fn) #Places output in func directory\n",
    "#     task = re.findall('task-(.*?_)', epi_3d_fn)[0][:-1] #output is TaskType_\n",
    "#     print(task)\n",
    "\n",
    "#     epi_hdr = nb.load(epi_3d_fn)\n",
    "#     vox_size = np.average(epi_hdr.header.get_zooms()).astype(int)\n",
    "\n",
    "#     in_fn = os.path.split(anat_fn)[-1]\n",
    "\n",
    "#     item_list = in_fn.split('T1w')\n",
    "#     item_list.insert(1, 'task-{}'.format(task))\n",
    "#     out_fn = ''.join(item_list)\n",
    "\n",
    "#     hdr = nb.load(anat_fn)\n",
    "\n",
    "#     out_name = os.path.join(out_path, out_fn)\n",
    "\n",
    "#     #Downsample\n",
    "#     anat_im_ds = nb.processing.resample_to_output(hdr, vox_size)\n",
    "#     nb.Nifti1Image(anat_im_ds.get_fdata(), header = anat_im_ds.header, affine = anat_im_ds.affine).to_filename(out_name)\n",
    "\n",
    "#     return(out_name)\n",
    "\n",
    "\n",
    "# def epi_parcellation(epi_fn, parc_fn, parc_ids_fn, atlas_name = 'aal'):\n",
    "\n",
    "#     '''\n",
    "#     Produces parcellation of functional data.\n",
    "#     Note: Functional and parcellation files must be the same dimensions.\n",
    "\n",
    "#     Input:\n",
    "#     epi_fn - Path to functional file to be parcellated\n",
    "#     parc_fn - Path to parcellation file. Should be a single 3d volume with unique IDs per parcellation\n",
    "#     parc_ids_fn - Path to text file with either numerical or anatomical labels for each parcellation\n",
    "#     atlas_name - Name of the atlas used for parcellataion\n",
    "\n",
    "#     Output:\n",
    "#     parc_vox_fn - Path to dictionary (in hdf5 form) that contains each voxel's time series from a parcellation\n",
    "#     parc_ts_fn - Path to .tsv file with mean timeseries from each parcellation region.\n",
    "#     '''\n",
    "\n",
    "#     out_fn_prefix = epi_fn.split('space')[0]\n",
    "#     out_fn_suffix = 'atlas-{}_timeseries'.format(atlas_name)\n",
    "#     out_fn_stem = '{}{}'.format(out_fn_prefix, out_fn_suffix)\n",
    "\n",
    "#     parc_vox = {}\n",
    "\n",
    "#     parc_ids = []\n",
    "#     with open(parc_ids_fn) as ids:\n",
    "#         for line in ids:\n",
    "#             parc_ids.append(line[:-1].lower()) #Remove newline\n",
    "\n",
    "#     hdr = nb.load(epi_fn)\n",
    "#     epi_data = hdr.get_fdata()\n",
    "\n",
    "#     parc = nb.load(parc_fn).get_fdata()\n",
    "#     parc_vals = np.unique(parc)[1:].astype(int)\n",
    "\n",
    "#     id_zip = zip(parc_vals, parc_ids)\n",
    "\n",
    "#     if epi_data.shape[:-1] != parc.shape:\n",
    "#         raise Exception('EPI and parcellation must be in same space')\n",
    "#     else:\n",
    "#         parc_ts = {}\n",
    "\n",
    "#     for n, (parc_n, parc_id) in enumerate(id_zip):\n",
    "#         roi_ts = epi_data[(parc == parc_n)]\n",
    "\n",
    "#         parc_vox[parc_id] = roi_ts\n",
    "#         parc_ts[parc_id] = roi_ts.mean(0)\n",
    "\n",
    "\n",
    "\n",
    "#     parc_vox_fn = '{}_parc_vox.h5'.format(out_fn_stem)\n",
    "#     parc_ts_fn = '{}_mean.tsv'.format(out_fn_stem)\n",
    "\n",
    "#     deepdish.io.save(parc_vox_fn, parc_vox)\n",
    "#     pd.DataFrame(parc_ts).to_csv(parc_ts_fn, sep = '\\t', index = False)\n",
    "\n",
    "#     return(parc_ts_fn)\n",
    "\n",
    "\n",
    "# def parc_lesion_overlap(parc_fn, lesion_fn, parc_ids_fn, atlas_name = 'aal'):\n",
    "#     '''\n",
    "#     Calculates overlap between lesion mask and parcellation atlas\n",
    "\n",
    "#     Input:\n",
    "#     parc_fn - Path to parcellation file. Should be a single 3d volume with unique IDs per parcellation\n",
    "#     lesion_fn - Path to lesion mask\n",
    "#     parc_ids_fn - Path to text file with either numerical or anatomical labels for each parcellation\n",
    "#     atlas_name - Name of the atlas used for parcellataion\n",
    "\n",
    "#     Output:\n",
    "#     out_fn - Path to tsv file showing amount of damage per parcel in sum of voxels and percentage of voxels\n",
    "#     '''\n",
    "\n",
    "#     out_fn_prefix = lesion_fn.split('space')[0]\n",
    "#     out_fn_suffix = 'atlas-{}_desc-lesion_damage.tsv'.format(atlas_name)\n",
    "#     out_fn = '{}{}'.format(out_fn_prefix, out_fn_suffix)\n",
    "\n",
    "#     parc_ids = []\n",
    "#     with open(parc_ids_fn) as ids:\n",
    "#         for line in ids:\n",
    "#             parc_ids.append(line[:-1].lower()) #Remove newline\n",
    "\n",
    "#     parc = nb.load(parc_fn).get_fdata()\n",
    "#     parc_vals = np.unique(parc)[1:].astype(int)\n",
    "\n",
    "#     lesion = nb.load(lesion_fn).get_fdata()\n",
    "\n",
    "#     if lesion.shape != parc.shape:\n",
    "#         raise Exception('Lesion mask and parcellation must be in same space')\n",
    "#     else:\n",
    "#         parc_damage = {}\n",
    "\n",
    "#     id_zip = zip(parc_vals, parc_ids)\n",
    "\n",
    "#     for n, (parc_n, parc_id) in enumerate(id_zip):\n",
    "#         parc_size = (parc == parc_n).sum()\n",
    "#         parc_damage[parc_id] = [int(lesion[parc == parc_n].sum()), abs(round(lesion[parc == parc_n].sum() / parc_size, 2))]\n",
    "\n",
    "#     parc_damage_df = pd.DataFrame(parc_damage).T\n",
    "#     parc_damage_df.columns = ['parc', 'sum', 'percent']\n",
    "#     parc_damage_df.to_csv(out_fn, sep = '\\t')\n",
    "\n",
    "#     return(out_fn)\n",
    "\n",
    "\n",
    "# def epi_calc_lag(epi_fn, gm_mask_fn, brain_mask_fn):\n",
    "#     '''\n",
    "#     Compute BOLD lag map (ala *insert paper here*).\n",
    "#     '''\n",
    "\n",
    "#     brain_mask = nb.load(brain_mask_fn).get_data().astype(bool)\n",
    "#     gm_mask = nb.load(out_name_ds).get_data().astype(bool)\n",
    "\n",
    "#     hdr = nb.load(epi_fn)\n",
    "#     epi_im = hdr.get_data()\n",
    "#     epi_ts = epi_im[brain_mask]\n",
    "#     tr = hdr.header.get_zooms()[-1]\n",
    "\n",
    "#     global_ts = epi_im[gm_mask].mean(0)\n",
    "\n",
    "#     epi_cross_corr = np.zeros(epi_ts.shape[0])\n",
    "#     nsamples = 1000\n",
    "\n",
    "\n",
    "#     out_path, in_fn = os.path.split(epi_fn)\n",
    "#     out_fn = in_fn.split('desc')[0] + 'desc-lag_map.nii.gz'\n",
    "\n",
    "\n",
    "#     for n, vox in enumerate(epi_ts):\n",
    "#         vox_zero_mean = vox - vox.mean(axis = 0)\n",
    "#         global_ts_zero_mean = global_ts - global_ts.mean(axis = 0)\n",
    "\n",
    "#         #Full cross correlation\n",
    "#     #     c = np.correlate(vox_zero_mean, global_ts_zero_mean, 'same') #Correlation of all shifts of time series.\n",
    "#         c = np.correlate(vox, global_ts, 'full') #Correlation of all shifts of time series.\n",
    "#         zero_lag = np.floor(c.shape[0] / 2).astype(int)\n",
    "#         c_culled = c[zero_lag - 3: zero_lag + 4] #Capture +- 3 TRs (9 seconds)\n",
    "#         maxC_idx = np.where(c == c_culled.max())[0].item() #Find where the maximum value of the windowed correlation exists within the larger cross correlation\n",
    "\n",
    "#         #Sectioned \n",
    "#         y = c[maxC_idx - 1: maxC_idx + 2] #Collect max +- 1 points for polynomial fit\n",
    "#         x = np.arange(0, len(y)) #Split 2d array into 1d\n",
    "#         z = np.polyfit(x, y, 2) #Second decree (parabolic) polynomial fit w/ coefs\n",
    "#         p = np.poly1d(z) # Calculate discrete vals\n",
    "#         xp = np.linspace(0, 2, nsamples) # Create array to house sample values\n",
    "#         interp = p(xp) # Calculate nsamples of poly func\n",
    "#         x_new = np.linspace(tr * -1, tr, p(xp).shape[0]) # Generate lag time series\n",
    "#         maxP_idx = np.where(interp == interp.max())[0].item() #Get max lag from interpolated time series\n",
    "\n",
    "#         #Putting it all back together\n",
    "#         lag_point = zero_lag - maxC_idx\n",
    "#         lag_point_seconds = tr * lag_point\n",
    "#         lag_val = np.round(lag_point_seconds + x_new[maxP_idx], 2)\n",
    "\n",
    "#         epi_cross_corr[n] = lag_val\n",
    "\n",
    "#     lag_map = np.zeros_like(brain_mask.astype(float))\n",
    "#     lag_map[brain_mask] = epi_cross_corr\n",
    "\n",
    "#     out_name = os.path.join(out_path, out_fn)\n",
    "\n",
    "#     lag_nii = nb.Nifti1Image(lag_map, header = hdr.header, affine = hdr.affine)\n",
    "#     lag_nii.to_filename(out_name)\n",
    "\n",
    "#     return(out_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdir = '/mnt/c/Users/pgoodin/Desktop/tia_100_out/fmriprep/'\n",
    "\n",
    "epi_fn = hdir + 'sub-tia001/func/sub-tia001_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'\n",
    "brain_mask_fn = hdir + 'sub-tia001/func/sub-tia001_task-rest_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz'\n",
    "\n",
    "fmriprep_confounds_fn = hdir + 'sub-tia001/func/sub-tia001_task-rest_desc-confounds_regressors.tsv'\n",
    "confounds_list = ['cosine00', 'cosine01','cosine02', 'cosine03', 'cosine04', 'cosine05', 'csf']\n",
    "\n",
    "mixing_matrix_fn = hdir + 'sub-tia001/func/sub-tia001_task-rest_desc-MELODIC_mixing.tsv'\n",
    "noise_comps_fn = hdir + 'sub-tia001/func/sub-tia001_task-rest_AROMAnoiseICs.csv'\n",
    "\n",
    "moving_fn = hdir + 'sub-tia001/anat/sub-tia001_desc-dseg.nii.gz'\n",
    "fixed_fn = hdir + 'sub-tia001/anat/sub-tia001_space-MNI152NLin2009cAsym_dseg.nii.gz'\n",
    "transform_fn = hdir + 'sub-tia001/anat/sub-tia001_from-T1w_to-MNI152NLin2009cAsym_mode-image_xfm.h5'\n",
    "\n",
    "parc_fn = './aal116/aal116MNI_3mm.nii.gz'\n",
    "parc_ids_fn = './aal116/aal116NodeNames.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epi_lesion_confound() #Lesion impact on ICA comps\n",
    "# epi_frist24() #Friston 24P motion\n",
    "# epi_collect_ICA_confounds() #AROMA id' motion + lesion\n",
    "# epi_gen_confounds_matrix() #Produce design matrix\n",
    "# epi_clean() #Run OLS, clean BOLD\n",
    "# epi_smooth() #Smooth data\n",
    "# anat_apply_transforms() #Warp from subj to MNI \n",
    "# anat_gm_mask() #Generate GM mask for lag calc (global specifically)\n",
    "# anat2epi_ds() #Take anat MNI warp and downsample to EPI matrix size\n",
    "# epi_parcellation() #Parcellate EPI \n",
    "# parc_lesion_overlap() #Calculate parcellations overlapping with lesion + amount damage per parcel\n",
    "# epi_calc_lag() #Calc lag map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lesion_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aaaceac16908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlesion_comps_fn\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mepi_lesion_confound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlesion_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmelodic_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_mask_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mselected_confounds_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepi_gen_confounds_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmriprep_confounds_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfounds_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maroma_confounds_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepi_collect_ICA_confounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixing_matrix_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_comps_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlesion_comps_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclean_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_fn_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepi_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepi_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_mask_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_confounds_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maroma_confounds_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# aal_fn = epi_parcellation(clean_fn[0], parc_fn, parc_ids_fn, atlas_name = 'aal')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lesion_fn' is not defined"
     ]
    }
   ],
   "source": [
    "lesion_comps_fn =  epi_lesion_confound(lesion_fn, melodic_fn, brain_mask_fn, thresh = 0.05)\n",
    "selected_confounds_fn = epi_gen_confounds_matrix(fmriprep_confounds_fn, confounds_list)\n",
    "aroma_confounds_fn = epi_collect_ICA_confounds(mixing_matrix_fn, noise_comps_fn, lesion_comps_fn)\n",
    "clean_fn, clean_fn_z = epi_clean(epi_fn, brain_mask_fn, selected_confounds_fn, aroma_confounds_fn, output_z = True)\n",
    "# aal_fn = epi_parcellation(clean_fn[0], parc_fn, parc_ids_fn, atlas_name = 'aal')\n",
    "\n",
    "\n",
    "# if len(clean_fn) > 1:\n",
    "#     smooth_fn = [epi_smooth(clean_single, mask_fn, 6) for clean_single in clean_fn]\n",
    "# else:\n",
    "#     smooth_fn = epi_smooth(clean_fn, mask_fn, 6)\n",
    "    \n",
    "# aparc_seg_fn = anat_apply_transforms(moving_fn, fixed_fn, transform_fn)\n",
    "# gm_mask_fn = anat_gm_mask(aparc_seg_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b2eff9b76e9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclean_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_fn' is not defined"
     ]
    }
   ],
   "source": [
    "clean_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/c/Users/pgoodin/Desktop/tia_100_out/fmriprep/sub-tia001/func/sub-tia001_task-rest_space-MNI152NLin2009cAsym_desc-preproc_cleaned.nii.gz',\n",
       " '/mnt/c/Users/pgoodin/Desktop/tia_100_out/fmriprep/sub-tia001/func/sub-tia001_task-rest_space-MNI152NLin2009cAsym_desc-preproc_cleaned_z.nii.gz')"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " hdr = nb.load(epi_fn)\n",
    "\n",
    "    brain_mask = nb.load(mask_fn).get_fdata().astype(bool)\n",
    "\n",
    "    if not fwhm:\n",
    "        fwhm = 0\n",
    "        print('\\n\\n***WARNING***\\nKernel FWHM not set!\\n\\n')\n",
    "\n",
    "    print('Smoothing with kernel size: {}'.format(fwhm))\n",
    "\n",
    "    smoothed_flat = nbproc.smooth_image(img = hdr, fwhm = fwhm).get_fdata()\n",
    "    smoothed_epi = np.zeros_like(hdr.get_fdata())\n",
    "    smoothed_epi[brain_mask] = smoothed_flat\n",
    "\n",
    "    out_path, in_fn = os.path.split(epi_fn)\n",
    "    out_fn = in_fn.split('.')[0].split('bold')[0] + '_smoothed_{}mm.nii.gz'.format(fwhm)\n",
    "    out_name = os.path.join(out_path, out_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/pgoodin/Desktop/tia_100_out/fmriprep/sub-tia001/anat/sub-tia001_space-MNI152NLin2009cAsym_desc-aparcaseg_dseg.nii.gz\n"
     ]
    }
   ],
   "source": [
    "moving_fn = hdir + 'sub-tia001/anat/sub-tia001_desc-aparcaseg_dseg.nii.gz'\n",
    "fixed_fn = hdir + 'sub-tia001/anat/sub-tia001_space-MNI152NLin2009cAsym_dseg.nii.gz'\n",
    "\n",
    "aparc_seg_fn = anat_apply_transforms(moving_fn, fixed_fn, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/pgoodin/Desktop/tia_100_out/fmriprep/sub-tia001/anat/sub-tia001_space-MNI152NLin2009cAsym_desc-gm_mask.nii.gz /mnt/c/Users/pgoodin/Desktop/tia_100_out/fmriprep/sub-tia001/func/sub-tia001_space-MNI152NLin2009cAsym_desc-gm_mask_voxsize_3mm.nii.gz\n"
     ]
    }
   ],
   "source": [
    "dseg_fn = hdir + 'sub-tia001/anat/sub-tia001_space-MNI152NLin2009cAsym_dseg.nii.gz'\n",
    "out_name, out_name_ds = anat_gm_mask(dseg_fn, 3)\n",
    "print(out_name, out_name_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lesion_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-26ac9132616a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlesion_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lesion_fn' is not defined"
     ]
    }
   ],
   "source": [
    "lesion_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_name = epi_calc_lag(clean_fn[1], out_name_ds, brain_mask_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 10 components associated with lesion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/pgoodin/Desktop/tia_100_out/fmriprep/sub-tia001/func/sub-tia001_task-rest_LesionICs.csv'"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi_lesion_confound(lesion_fn, melodic_fn, brain_mask_fn, thresh = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 35, 36, 37, 38, 39,\n",
       "       41, 42, 43, 45, 46, 47, 48, 49, 51])"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
